{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cad2b49",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "## Sentence tokenizer; sentence terminate either in (, ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc36b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic regression is often mentioned in connection to classification tasks. \n",
      "The model is simple and one of the easy starters to learn about generating probabilities, \n",
      "classifying samples, and understanding gradient descent. \n",
      "This tutorial walks you through some mathematical equations and pairs them with practical examples in Python \n",
      "so that you can see exactly how to train your own custom binary logistic regression model.\n",
      "\n",
      "['\\nLogistic regression is often mentioned in connection to classification tasks', ' \\nThe model is simple and one of the easy starters to learn about generating probabilities, \\nclassifying samples, and understanding gradient descent', ' \\nThis tutorial walks you through some mathematical equations and pairs them with practical examples in Python \\nso that you can see exactly how to train your own custom binary logistic regression model', '\\n']\n"
     ]
    }
   ],
   "source": [
    "# src: https://medium.com/@koushikkushal95/logistic-regression-from-scratch-dfb8527a4226\n",
    "paragraph = \"\"\"\n",
    "Logistic regression is often mentioned in connection to classification tasks. \n",
    "The model is simple and one of the easy starters to learn about generating probabilities, \n",
    "classifying samples, and understanding gradient descent. \n",
    "This tutorial walks you through some mathematical equations and pairs them with practical examples in Python \n",
    "so that you can see exactly how to train your own custom binary logistic regression model.\n",
    "\"\"\"\n",
    "print(paragraph)\n",
    "\n",
    "# define the delimeters\n",
    "delimeters = ['.', '?', '!']\n",
    "\n",
    "\n",
    "# split the sentence by space\n",
    "sentencesTokens = paragraph.split(\".\")\n",
    "print(sentencesTokens)\n",
    "# split the words in the sentence\n",
    "words=[]\n",
    "\n",
    "# Print the sentences in the paragraph\n",
    "# for sentence in sentences:\n",
    "#     print(sentence)\n",
    "#     for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32af685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Hello', 'I am learning Python', \"It's really fun, isn't it\", 'Yes, indeed']\n",
      "Words: ['Hello', 'I', 'am', 'learning', 'Python', 'Its', 'really', 'fun', 'isnt', 'it', 'Yes', 'indeed']\n",
      "Tokens: ['Hello', '!', 'I', 'am', 'learning', 'Python', '.', 'It', \"'\", 's', 'really', 'fun', ',', 'isn', \"'\", 't', 'it', '?', 'Yes', ',', 'indeed', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence  word tokenizer\n",
    "import string\n",
    "\n",
    "# Sample paragraph\n",
    "paragraph = \"\"\"Hello! I am learning Python. It's really fun, isn't it? Yes, indeed.\"\"\"\n",
    "\n",
    "# 1. Sentence Tokenizer (manually splitting using delimiters)\n",
    "def manual_sentence_tokenizer(text):\n",
    "    import re\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    return [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "# 2. Word Tokenizer (remove punctuation, then split by space)\n",
    "def manual_word_tokenizer(text):\n",
    "    clean_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    words = clean_text.split()\n",
    "    return words\n",
    "\n",
    "# 3. Tokenizer (words + punctuation tokens separately)\n",
    "def manual_tokenizer(text):\n",
    "    tokens = []\n",
    "    word = ''\n",
    "    for char in text:\n",
    "        if char in string.whitespace:\n",
    "            if word:\n",
    "                tokens.append(word)\n",
    "                word = ''\n",
    "        elif char in string.punctuation:\n",
    "            if word:\n",
    "                tokens.append(word)\n",
    "                word = ''\n",
    "            tokens.append(char)\n",
    "        else:\n",
    "            word += char\n",
    "    if word:\n",
    "        tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "# Output\n",
    "sentences = manual_sentence_tokenizer(paragraph)\n",
    "words = manual_word_tokenizer(paragraph)\n",
    "tokens = manual_tokenizer(paragraph)\n",
    "\n",
    "print(\"Sentences:\", sentences)\n",
    "print(\"Words:\", words)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e39a624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Hello!', 'I am learning Python.', \"It's really fun, isn't it?\", 'Yes, indeed.']\n",
      "Words: ['Hello', 'I', 'am', 'learning', 'Python', 'Its', 'really', 'fun', 'isnt', 'it', 'Yes', 'indeed']\n",
      "Tokens: ['Hello', '!', 'I', 'am', 'learning', 'Python', '.', 'It', \"'\", 's', 'really', 'fun', ',', 'isn', \"'\", 't', 'it', '?', 'Yes', ',', 'indeed', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sentence, word tokenizer manual no imports\n",
    "import string\n",
    "\n",
    "# Input paragraph\n",
    "paragraph = \"Hello! I am learning Python. It's really fun, isn't it? Yes, indeed.\"\n",
    "\n",
    "# 1. Manual Sentence Tokenizer (split at ., !, ?)\n",
    "def sentence_tokenizer(text):\n",
    "    sentences = []\n",
    "    sentence = ''\n",
    "    end_marks = {'.', '!', '?'}\n",
    "    for char in text:\n",
    "        sentence += char\n",
    "        if char in end_marks:\n",
    "            sentences.append(sentence.strip())\n",
    "            sentence = ''\n",
    "    if sentence:  # Any leftover\n",
    "        sentences.append(sentence.strip())\n",
    "    return sentences\n",
    "\n",
    "# 2. Manual Word Tokenizer (remove punctuation, then split by spaces)\n",
    "def word_tokenizer(text):\n",
    "    words = []\n",
    "    word = ''\n",
    "    for char in text:\n",
    "        if char in string.whitespace:\n",
    "            if word:\n",
    "                words.append(word)\n",
    "                word = ''\n",
    "        elif char in string.punctuation:\n",
    "            continue\n",
    "        else:\n",
    "            word += char\n",
    "    if word:\n",
    "        words.append(word)\n",
    "    return words\n",
    "\n",
    "# 3. Manual General Tokenizer (words + punctuation as separate tokens)\n",
    "def general_tokenizer(text):\n",
    "    tokens = []\n",
    "    word = ''\n",
    "    for char in text:\n",
    "        if char in string.whitespace:\n",
    "            if word:\n",
    "                tokens.append(word)\n",
    "                word = ''\n",
    "        elif char in string.punctuation:\n",
    "            if word:\n",
    "                tokens.append(word)\n",
    "                word = ''\n",
    "            tokens.append(char)\n",
    "        else:\n",
    "            word += char\n",
    "    if word:\n",
    "        tokens.append(word)\n",
    "    return tokens\n",
    "\n",
    "# Run tokenizers\n",
    "sentences = sentence_tokenizer(paragraph)\n",
    "words = word_tokenizer(paragraph)\n",
    "tokens = general_tokenizer(paragraph)\n",
    "\n",
    "# Output\n",
    "print(\"Sentences:\", sentences)\n",
    "print(\"Words:\", words)\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
