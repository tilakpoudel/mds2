{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGZWw9a_JEzt"
      },
      "outputs": [],
      "source": [
        "# Scratch implementation of artifical neural network (ANN) using some toy datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9733e1b4"
      },
      "source": [
        "# Task\n",
        "Implement an Artificial Neural Network (ANN) from scratch by following the provided plan, which includes defining the network architecture, initializing parameters, implementing activation functions, forward propagation, loss function, backward propagation, parameter updates, creating a training loop, generating a toy dataset, and finally training and evaluating the ANN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85594c57"
      },
      "source": [
        "## Define Network Architecture\n",
        "\n",
        "Outline the structure of the ANN, including the number of layers, neurons per layer, and activation functions for each layer. This will set the foundation for our network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c791afa"
      },
      "source": [
        "### Network Architecture Definition\n",
        "\n",
        "For our scratch implementation of an Artificial Neural Network (ANN) using toy datasets, we will define a simple, yet illustrative, network architecture:\n",
        "\n",
        "1.  **Input Layer**: The size of the input layer will depend on the number of features in our toy dataset. For typical toy datasets, this could be between 2 to 4 features.\n",
        "2.  **Hidden Layers**: We will implement **one hidden layer**.\n",
        "    *   **Hidden Layer 1**: This layer will consist of **4 neurons**. A common practice is to use a number of neurons between the input and output layer sizes, often powers of 2. The **activation function** for this layer will be the **sigmoid function**, as it's a classic choice for hidden layers in simpler ANNs.\n",
        "3.  **Output Layer**: The size of the output layer will depend on the nature of the classification task.\n",
        "    *   For a **binary classification** problem, the output layer will have **1 neuron**. The **activation function** for the output layer will also be the **sigmoid function**, as it naturally outputs values between 0 and 1, suitable for binary classification probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "045f5633"
      },
      "source": [
        "## Initialize Parameters\n",
        "\n",
        "Write code to randomly initialize the weights and biases for all layers of the neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6a4efb4"
      },
      "source": [
        "The task is to initialize the weights and biases of a neural network. We need to define a Python function `initialize_parameters` that takes the network's architecture (input, hidden, output sizes) and uses `numpy` to generate random weights and zero biases, storing them in a dictionary.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0cde77c",
        "outputId": "17d93ee3-7113-48f3-ff49-1f5247662acc"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialize_parameters(input_size, hidden_size, output_size):\n",
        "    \"\"\"\n",
        "    Initializes the weights and biases for a two-layer neural network.\n",
        "\n",
        "    Arguments:\n",
        "    input_size -- size of the input layer\n",
        "    hidden_size -- size of the hidden layer\n",
        "    output_size -- size of the output layer\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    \"W1\" -- weight matrix of shape (hidden_size, input_size)\n",
        "                    \"b1\" -- bias vector of shape (hidden_size, 1)\n",
        "                    \"W2\" -- weight matrix of shape (output_size, hidden_size)\n",
        "                    \"b2\" -- bias vector of shape (output_size, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize W1 with small random values\n",
        "    W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
        "    # Initialize b1 with zeros\n",
        "    b1 = np.zeros((hidden_size, 1))\n",
        "    # Initialize W2 with small random values\n",
        "    W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
        "    # Initialize b2 with zeros\n",
        "    b2 = np.zeros((output_size, 1))\n",
        "\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters\n",
        "\n",
        "print(\"Parameters initialization function defined.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters initialization function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d424f39"
      },
      "source": [
        "## Implement Activation Functions\n",
        "\n",
        "Define common activation functions like sigmoid, ReLU, or tanh, which will be used in the network's layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e803915a"
      },
      "source": [
        "First, we will execute the existing sigmoid function definition to ensure it's loaded into the environment. Then, we will define the ReLU activation function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bc961e77"
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28d02337"
      },
      "source": [
        "Now that the sigmoid function is executed, we will define the `relu` function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a465bf9b",
        "outputId": "dda78fbf-766d-4ca5-b0cb-4d94ede5f400"
      },
      "source": [
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "print(\"ReLU function defined.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ReLU function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16a5addb"
      },
      "source": [
        "## Implement Forward Propagation\n",
        "\n",
        "Develop the forward pass mechanism, where input data is fed through the network to produce an output. This involves calculating weighted sums and applying activation functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec26485f"
      },
      "source": [
        "We need to define the `forward_propagation` function as described in the instructions, which involves calculating weighted sums and applying activation functions for both the hidden and output layers, and storing intermediate values in a cache.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7a7dd68",
        "outputId": "7745e68a-db1f-4c38-ac49-f69a0ac91694"
      },
      "source": [
        "def forward_propagation(X, parameters):\n",
        "    \"\"\"\n",
        "    Implements the forward propagation for the model:\n",
        "    LINEAR -> SIGMOID -> LINEAR -> SIGMOID\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data of shape (input_size, number of examples)\n",
        "    parameters -- python dictionary containing your parameters (\"W1\", \"b1\", \"W2\", \"b2\")\n",
        "\n",
        "    Returns:\n",
        "    A2 -- The sigmoid output of the second activation\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve parameters from the dictionary\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "    # Linear combination for the first layer (hidden layer)\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    # Apply sigmoid activation\n",
        "    A1 = sigmoid(Z1)\n",
        "\n",
        "    # Linear combination for the second layer (output layer)\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    # Apply sigmoid activation for the final output\n",
        "    A2 = sigmoid(Z2)\n",
        "\n",
        "    # Store intermediate values in cache for backpropagation\n",
        "    cache = {\"Z1\": Z1,\n",
        "             \"A1\": A1,\n",
        "             \"Z2\": Z2,\n",
        "             \"A2\": A2}\n",
        "\n",
        "    return A2, cache\n",
        "\n",
        "print(\"Forward propagation function defined.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forward propagation function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "533498c7"
      },
      "source": [
        "## Implement Loss Function\n",
        "\n",
        "Choose and implement a suitable loss function (e.g., Mean Squared Error for regression or Cross-Entropy for classification) to measure the difference between predicted and actual outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8f8912f"
      },
      "source": [
        "We need to implement the `compute_cost` function, which will calculate the binary cross-entropy loss between the predicted output `A2` and the true labels `Y`, handling potential `log(0)` errors and averaging the loss over all examples.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "192d0a1a",
        "outputId": "c9ca488e-2fda-4eac-ca54-fa3aa8d3fb4a"
      },
      "source": [
        "def compute_cost(A2, Y):\n",
        "    \"\"\"\n",
        "    Computes the binary cross-entropy cost.\n",
        "\n",
        "    Arguments:\n",
        "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "\n",
        "    m = Y.shape[1]  # Number of examples\n",
        "\n",
        "    # Add a small epsilon to prevent log(0) errors\n",
        "    epsilon = 1e-8\n",
        "    A2 = np.clip(A2, epsilon, 1 - epsilon)\n",
        "\n",
        "    # Compute the cross-entropy cost\n",
        "    cost = - (1/m) * np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2))\n",
        "\n",
        "    # Ensure cost is a single number\n",
        "    cost = np.squeeze(cost)\n",
        "\n",
        "    return cost\n",
        "\n",
        "print(\"Cost function defined.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cost function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2afabc0e"
      },
      "source": [
        "## Implement Backward Propagation (Backpropagation)\n",
        "\n",
        "Develop the backpropagation algorithm to calculate the gradients of the loss function with respect to the network's weights and biases. This is essential for learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc607825"
      },
      "source": [
        "We need to define the `backward_propagation` function as per the instructions, which involves calculating the gradients of the loss function with respect to the network's weights and biases (dW1, db1, dW2, db2) using the provided formulas and intermediate values from the cache.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54874372",
        "outputId": "351b8bc3-3b56-4d20-be9d-1c5a417480fa"
      },
      "source": [
        "def backward_propagation(parameters, cache, X, Y):\n",
        "    \"\"\"\n",
        "    Implements the backward propagation for the model.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing our parameters\n",
        "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
        "    X -- input data of shape (input_size, number of examples)\n",
        "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    grads -- python dictionary containing your gradients with respect to different parameters:\n",
        "             \"dW1\" -- gradient of cost with respect to W1\n",
        "             \"db1\" -- gradient of cost with respect to b1\n",
        "             \"dW2\" -- gradient of cost with respect to W2\n",
        "             \"db2\" -- gradient of cost with respect to b2\n",
        "    \"\"\"\n",
        "    m = X.shape[1] # Number of examples\n",
        "\n",
        "    # Retrieve W1 and W2 from parameters\n",
        "    W1 = parameters[\"W1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "\n",
        "    # Retrieve A1, A2, Z1, Z2 from cache\n",
        "    A1 = cache[\"A1\"]\n",
        "    A2 = cache[\"A2\"]\n",
        "    Z1 = cache[\"Z1\"]\n",
        "    Z2 = cache[\"Z2\"]\n",
        "\n",
        "    # Backward propagation for the output layer\n",
        "    dZ2 = A2 - Y\n",
        "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "    # Backward propagation for the hidden layer\n",
        "    dA1 = np.dot(W2.T, dZ2)\n",
        "    # Sigmoid gradient: A1 * (1 - A1)\n",
        "    dZ1 = dA1 * (A1 * (1 - A1))\n",
        "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
        "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    grads = {\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\": dW2,\n",
        "             \"db2\": db2}\n",
        "\n",
        "    return grads\n",
        "\n",
        "print(\"Backward propagation function defined.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Backward propagation function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9994db93"
      },
      "source": [
        "## Implement Parameter Update (Optimization)\n",
        "\n",
        "Write code to update the network's weights and biases using an optimization algorithm like Gradient Descent, based on the gradients computed during backpropagation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "329c8c0d"
      },
      "source": [
        "We need to define the `update_parameters` function as instructed, which will apply the gradient descent update rule to the network's weights and biases using the calculated gradients and a specified learning rate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "717044c0",
        "outputId": "e462fce6-5951-4eb4-a985-c450d79851e7"
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Updates parameters using the gradient descent update rule.\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters\n",
        "    grads -- python dictionary containing your gradients\n",
        "    learning_rate -- the learning rate, a scalar\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve current parameters\n",
        "    W1 = parameters[\"W1\"]\n",
        "    b1 = parameters[\"b1\"]\n",
        "    W2 = parameters[\"W2\"]\n",
        "    b2 = parameters[\"b2\"]\n",
        "\n",
        "    # Retrieve gradients\n",
        "    dW1 = grads[\"dW1\"]\n",
        "    db1 = grads[\"db1\"]\n",
        "    dW2 = grads[\"dW2\"]\n",
        "    db2 = grads[\"db2\"]\n",
        "\n",
        "    # Update rule for each parameter\n",
        "    W1 = W1 - learning_rate * dW1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "    W2 = W2 - learning_rate * dW2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "\n",
        "    # Store updated parameters\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters\n",
        "\n",
        "print(\"Parameters update function defined.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameters update function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d8e9229"
      },
      "source": [
        "## Create Training Loop\n",
        "\n",
        "Combine forward propagation, loss calculation, backpropagation, and parameter updates into an iterative training loop that runs for a specified number of epochs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b1b2e25"
      },
      "source": [
        "We need to define the `nn_model` function to encapsulate the entire training loop as described in the instructions. This function will call the previously defined functions for initialization, forward propagation, cost computation, backward propagation, and parameter updates, and will also record the cost over iterations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed3a10b9",
        "outputId": "4afa9c6c-d7ec-4b92-8d97-f7d7b2bf4d33"
      },
      "source": [
        "def nn_model(X, Y, n_h, num_iterations, learning_rate, print_cost=False):\n",
        "    \"\"\"\n",
        "    Implements a two-layer neural network model.\n",
        "\n",
        "    Arguments:\n",
        "    X -- input data of shape (input_size, number of examples)\n",
        "    Y -- true \"label\" vector of shape (1, number of examples)\n",
        "    n_h -- size of the hidden layer\n",
        "    num_iterations -- Number of iterations to run for gradient descent\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- if True, print the cost every 100 iterations\n",
        "\n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    costs -- list of costs recorded during training\n",
        "    \"\"\"\n",
        "\n",
        "    n_x = X.shape[0]  # input layer size\n",
        "    n_y = Y.shape[0]  # output layer size (fixed to 1 for binary classification)\n",
        "\n",
        "    # 1. Initialize parameters\n",
        "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
        "\n",
        "    # Lists to store costs for plotting\n",
        "    costs = []\n",
        "\n",
        "    # 2. Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # a. Forward propagation\n",
        "        A2, cache = forward_propagation(X, parameters)\n",
        "\n",
        "        # b. Compute cost\n",
        "        cost = compute_cost(A2, Y)\n",
        "\n",
        "        # c. Backward propagation\n",
        "        grads = backward_propagation(parameters, cache, X, Y)\n",
        "\n",
        "        # d. Update parameters\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "        # e. Optionally, print the cost and store it\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(f\"Cost after iteration {i}: {cost}\")\n",
        "            costs.append(cost)\n",
        "\n",
        "    return parameters, costs\n",
        "\n",
        "print(\"Neural network model training function defined.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Neural network model training function defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d96d8656"
      },
      "source": [
        "## Generate Toy Dataset\n",
        "\n",
        "Create a simple synthetic dataset (e.g., for binary classification or regression) to test our ANN implementation. This will allow us to easily visualize and debug the network's performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc0ded05"
      },
      "source": [
        "We need to generate a synthetic dataset for binary classification using `make_moons`, reshape it according to the ANN input requirements, and then print the shapes of the resulting `X` and `Y` to verify the transformation. This addresses all parts of the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f47190fe",
        "outputId": "8d244065-d084-4cd7-83d1-4ed01816a289"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "# 1. Generate a dataset using make_moons\n",
        "X, Y = make_moons(n_samples=200, noise=0.20, random_state=42)\n",
        "\n",
        "# 2. Reshape X and Y to be compatible with the ANN input and output requirements\n",
        "# X should be of shape (number_of_features, number_of_examples)\n",
        "# Y should be of shape (1, number_of_examples)\n",
        "X = X.T\n",
        "Y = Y.reshape(1, Y.shape[0])\n",
        "\n",
        "# 3. Print the shapes of X and Y to verify the reshaping\n",
        "print(f\"Shape of X: {X.shape}\")\n",
        "print(f\"Shape of Y: {Y.shape}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (2, 200)\n",
            "Shape of Y: (1, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a891184a"
      },
      "source": [
        "## Train and Evaluate the ANN\n",
        "\n",
        "Train the implemented ANN using the toy dataset and evaluate its performance using appropriate metrics (e.g., accuracy, precision, recall, or MSE) to ensure it's learning correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8693056f"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Network Architecture Defined**: A two-layer Artificial Neural Network (ANN) was defined, consisting of an input layer, one hidden layer with 4 neurons and a sigmoid activation, and an output layer with 1 neuron and a sigmoid activation for binary classification.\n",
        "*   **Core ANN Components Implemented**: All essential components for an ANN were successfully implemented from scratch:\n",
        "    *   **Parameter Initialization**: A function `initialize_parameters` was created to set initial weights with small random values and biases to zeros.\n",
        "    *   **Activation Functions**: `sigmoid` and `relu` functions were defined, with `sigmoid` being used for both hidden and output layers in the final model.\n",
        "    *   **Forward Propagation**: The `forward_propagation` function correctly calculates weighted sums and applies activation functions across layers, storing intermediate values in a `cache`.\n",
        "    *   **Loss Function**: A `compute_cost` function was implemented for binary cross-entropy loss, incorporating numerical stability with an epsilon value of $1\\text{e}-8$.\n",
        "    *   **Backward Propagation**: The `backward_propagation` function accurately computes gradients for all weights and biases, utilizing the cached values and applying the sigmoid gradient.\n",
        "    *   **Parameter Update**: An `update_parameters` function was developed to update network weights and biases using the gradient descent rule with a specified learning rate.\n",
        "*   **Training Loop Established**: A comprehensive `nn_model` function was created, integrating all implemented components into an iterative training loop. This function handles parameter initialization, forward and backward propagation, cost computation, and parameter updates over a specified number of iterations.\n",
        "*   **Toy Dataset Generated**: A synthetic binary classification dataset (200 samples, `make_moons` with $0.20$ noise) was successfully generated and reshaped to be compatible with the ANN's input (`X` of shape (2, 200)) and output (`Y` of shape (1, 200)) requirements.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The foundational components of an ANN have been successfully implemented and integrated, providing a robust base for further development and experimentation.\n",
        "*   The next logical step is to train the implemented ANN using the generated toy dataset and then evaluate its performance using appropriate metrics (e.g., accuracy, precision, recall) to ensure it's learning correctly and to identify potential areas for optimization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f76d9664"
      },
      "source": [
        "## Train and Evaluate the ANN\n",
        "\n",
        "Train the implemented ANN using the toy dataset and evaluate its performance using appropriate metrics (e.g., accuracy, precision, recall, or MSE) to ensure it's learning correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20850a3e"
      },
      "source": [
        "We need to define a `predict` function that takes the trained parameters and input data `X` to make predictions. This function will use the forward propagation logic and then threshold the output `A2` at 0.5 to classify the data. After defining the predict function, I will execute the `nn_model` function to train the neural network, then make predictions on the `X` dataset and calculate the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bf59e08",
        "outputId": "f1bb3e27-45ad-419f-ba24-9f7ef9f7d390"
      },
      "source": [
        "def predict(parameters, X):\n",
        "    \"\"\"\n",
        "    Using the learned parameters, predicts a class for each example in X\n",
        "\n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters\n",
        "    X -- input data of size (input_size, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    predictions -- vector of predictions of our model (0 or 1)\n",
        "    \"\"\"\n",
        "\n",
        "    # Forward propagation to get A2 (probabilities)\n",
        "    A2, cache = forward_propagation(X, parameters)\n",
        "\n",
        "    # Convert probabilities to actual predictions (0 or 1)\n",
        "    predictions = (A2 > 0.5).astype(int)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Set hyperparameters\n",
        "input_size = X.shape[0]\n",
        "hidden_size = 4 # As defined in the network architecture\n",
        "output_size = Y.shape[0]\n",
        "num_iterations = 10000\n",
        "learning_rate = 0.05\n",
        "\n",
        "print(\"Starting ANN training...\")\n",
        "# Train the model\n",
        "parameters, costs = nn_model(X, Y, n_h=hidden_size, num_iterations=num_iterations, learning_rate=learning_rate, print_cost=True)\n",
        "\n",
        "print(\"Training complete. Making predictions...\")\n",
        "# Make predictions\n",
        "predictions = predict(parameters, X)\n",
        "\n",
        "# Calculate accuracy, explicitly extracting scalar from dot products to avoid DeprecationWarning\n",
        "accuracy = (np.dot(Y, predictions.T).item() + np.dot(1 - Y, 1 - predictions.T).item()) / Y.size * 100\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ANN training...\n",
            "Cost after iteration 0: 0.6931551143394612\n",
            "Cost after iteration 100: 0.6931456321884061\n",
            "Cost after iteration 200: 0.6931362854282024\n",
            "Cost after iteration 300: 0.6931203760741177\n",
            "Cost after iteration 400: 0.6930880440425858\n",
            "Cost after iteration 500: 0.6930192223815069\n",
            "Cost after iteration 600: 0.692871119565736\n",
            "Cost after iteration 700: 0.6925517320849316\n",
            "Cost after iteration 800: 0.6918639221017446\n",
            "Cost after iteration 900: 0.6903905521471135\n",
            "Cost after iteration 1000: 0.6872739467718904\n",
            "Cost after iteration 1100: 0.6808569791753908\n",
            "Cost after iteration 1200: 0.6683347425296517\n",
            "Cost after iteration 1300: 0.646136950449808\n",
            "Cost after iteration 1400: 0.612185851573386\n",
            "Cost after iteration 1500: 0.5690696753796732\n",
            "Cost after iteration 1600: 0.5236986548719255\n",
            "Cost after iteration 1700: 0.4826664387840508\n",
            "Cost after iteration 1800: 0.44897540964052096\n",
            "Cost after iteration 1900: 0.4225778742743356\n",
            "Cost after iteration 2000: 0.40216931781684573\n",
            "Cost after iteration 2100: 0.38632140068391885\n",
            "Cost after iteration 2200: 0.37387380494123745\n",
            "Cost after iteration 2300: 0.36397511285534123\n",
            "Cost after iteration 2400: 0.3560196170474892\n",
            "Cost after iteration 2500: 0.34957497588278086\n",
            "Cost after iteration 2600: 0.34432593360543096\n",
            "Cost after iteration 2700: 0.3400358233283858\n",
            "Cost after iteration 2800: 0.3365217127256882\n",
            "Cost after iteration 2900: 0.33363873367890096\n",
            "Cost after iteration 3000: 0.33127017644276097\n",
            "Cost after iteration 3100: 0.3293210458789293\n",
            "Cost after iteration 3200: 0.3277136471258897\n",
            "Cost after iteration 3300: 0.32638436241346536\n",
            "Cost after iteration 3400: 0.32528115340108615\n",
            "Cost after iteration 3500: 0.3243615404510322\n",
            "Cost after iteration 3600: 0.3235909277520668\n",
            "Cost after iteration 3700: 0.3229412021534323\n",
            "Cost after iteration 3800: 0.3223895608978933\n",
            "Cost after iteration 3900: 0.3219175353112383\n",
            "Cost after iteration 4000: 0.321510182729127\n",
            "Cost after iteration 4100: 0.321155421830801\n",
            "Cost after iteration 4200: 0.3208434889155201\n",
            "Cost after iteration 4300: 0.32056649507735996\n",
            "Cost after iteration 4400: 0.32031806675759184\n",
            "Cost after iteration 4500: 0.3200930546645078\n",
            "Cost after iteration 4600: 0.3198872984197731\n",
            "Cost after iteration 4700: 0.31969743643007303\n",
            "Cost after iteration 4800: 0.3195207523510121\n",
            "Cost after iteration 4900: 0.3193550511007248\n",
            "Cost after iteration 5000: 0.3191985587098644\n",
            "Cost after iteration 5100: 0.31904984139062914\n",
            "Cost after iteration 5200: 0.31890774010253137\n",
            "Cost after iteration 5300: 0.3187713176186418\n",
            "Cost after iteration 5400: 0.3186398156822419\n",
            "Cost after iteration 5500: 0.3185126203156604\n",
            "Cost after iteration 5600: 0.3183892337221177\n",
            "Cost after iteration 5700: 0.3182692515255428\n",
            "Cost after iteration 5800: 0.318152344337217\n",
            "Cost after iteration 5900: 0.3180382428336345\n",
            "Cost after iteration 6000: 0.3179267256867716\n",
            "Cost after iteration 6100: 0.3178176098137405\n",
            "Cost after iteration 6200: 0.3177107425138045\n",
            "Cost after iteration 6300: 0.31760599514190196\n",
            "Cost after iteration 6400: 0.3175032580331786\n",
            "Cost after iteration 6500: 0.31740243644572774\n",
            "Cost after iteration 6600: 0.3173034473313514\n",
            "Cost after iteration 6700: 0.31720621677869343\n",
            "Cost after iteration 6800: 0.31711067800119064\n",
            "Cost after iteration 6900: 0.317016769765209\n",
            "Cost after iteration 7000: 0.3169244351725011\n",
            "Cost after iteration 7100: 0.31683362072654697\n",
            "Cost after iteration 7200: 0.3167442756250504\n",
            "Cost after iteration 7300: 0.3166563512313711\n",
            "Cost after iteration 7400: 0.3165698006863733\n",
            "Cost after iteration 7500: 0.31648457862938656\n",
            "Cost after iteration 7600: 0.31640064100295623\n",
            "Cost after iteration 7700: 0.3163179449210197\n",
            "Cost after iteration 7800: 0.31623644858424454\n",
            "Cost after iteration 7900: 0.316156111229648\n",
            "Cost after iteration 8000: 0.3160768931043944\n",
            "Cost after iteration 8100: 0.3159987554559378\n",
            "Cost after iteration 8200: 0.3159216605325224\n",
            "Cost after iteration 8300: 0.3158455715895394\n",
            "Cost after iteration 8400: 0.31577045289842937\n",
            "Cost after iteration 8500: 0.31569626975575965\n",
            "Cost after iteration 8600: 0.3156229884908454\n",
            "Cost after iteration 8700: 0.3155505764708518\n",
            "Cost after iteration 8800: 0.31547900210274743\n",
            "Cost after iteration 8900: 0.3154082348318061\n",
            "Cost after iteration 9000: 0.31533824513658637\n",
            "Cost after iteration 9100: 0.3152690045204876\n",
            "Cost after iteration 9200: 0.31520048550009366\n",
            "Cost after iteration 9300: 0.31513266159058684\n",
            "Cost after iteration 9400: 0.3150655072885559\n",
            "Cost after iteration 9500: 0.3149989980525378\n",
            "Cost after iteration 9600: 0.31493311028163457\n",
            "Cost after iteration 9700: 0.3148678212925367\n",
            "Cost after iteration 9800: 0.31480310929526334\n",
            "Cost after iteration 9900: 0.3147389533679081\n",
            "Training complete. Making predictions...\n",
            "Accuracy: 84.50%\n"
          ]
        }
      ]
    }
  ]
}