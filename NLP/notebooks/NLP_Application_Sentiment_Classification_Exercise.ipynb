{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjbWuEbVobat"
      },
      "source": [
        "# Sentiment Analysis using Bag of words\n",
        "\n",
        "\n",
        "\n",
        "Sentiment analysis is to analyze the textual documents and extract information that is related to the author’s sentiment or opinion. It is sometimes referred to as opinion mining.\n",
        "\n",
        "It is popular and widely used in industry, e.g., corporate surveys, feedback surveys, social media data, reviews for movies, places, hotels, commodities, etc..\n",
        "\n",
        "The sentiment information from texts can be crucial to further decision making in the industry.\n",
        "\n",
        "**Output of Sentiment Analysis**\n",
        "\n",
        "- Qualitative: overall sentiment scale (positive/negative)\n",
        "- Quantitative: sentiment polarity scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0JS-mtYsWr9"
      },
      "source": [
        "## Dataset : `NepCov19Tweet`\n",
        "\n",
        "Referece: C Sitaula, A Basnet, A Mainali and TB Shahi, **Deep Learning-based Methods for Sentiment Analysis on Nepali COVID-19-related Tweets**, Computational Intelligence and Neuroscience, 2021. [Link](https://onlinelibrary.wiley.com/doi/full/10.1155/2021/2158184)\n",
        "\n",
        "Source: https://www.kaggle.com/datasets/mathew11111/nepcov19tweets/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bnfj458VsXY-"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Label</th>\n",
              "      <th>Datetime</th>\n",
              "      <th>Tweet</th>\n",
              "      <th>Tokanize_tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>2021-01-10 22:06:41+00:00</td>\n",
              "      <td>अमेरिकामा कोभिड बाट एकै दिन चार हजारभन्दा बढीक...</td>\n",
              "      <td>अमेरिकामा,कोभिड,बाट,एकै,दिन,चार,हजारभन्दा,बढीक...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>2021-01-10 17:49:34+00:00</td>\n",
              "      <td>कोभिड का कारण विदेशमा रहेका नेपालीहरुमा मानसिक...</td>\n",
              "      <td>कोभिड,का,कारण,विदेशमा,रहेका,नेपालीहरुमा,मानसिक...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2021-01-10 16:18:34+00:00</td>\n",
              "      <td>नेपालमा क्लोभर बायोफार्मास्युटिकल्स अस्ट्रेलिय...</td>\n",
              "      <td>नेपालमा,क्लोभर,बायोफार्मास्युटिकल्स,अस्ट्रेलिय...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2021-01-10 15:12:17+00:00</td>\n",
              "      <td>कोभिड को खोप पनि लगाइयो</td>\n",
              "      <td>कोभिड,को,खोप,पनि,लगाइयो</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>2021-01-10 15:07:12+00:00</td>\n",
              "      <td>अमेरिकामा कोभिड को नयाँ रेकर्ड एकै दिन हजारभन्...</td>\n",
              "      <td>अमेरिकामा,कोभिड,को,नयाँ,रेकर्ड,एकै,दिन,हजारभन्...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0 Label                   Datetime  \\\n",
              "0           0    -1  2021-01-10 22:06:41+00:00   \n",
              "1           1    -1  2021-01-10 17:49:34+00:00   \n",
              "2           2     1  2021-01-10 16:18:34+00:00   \n",
              "3           3     0  2021-01-10 15:12:17+00:00   \n",
              "4           4    -1  2021-01-10 15:07:12+00:00   \n",
              "\n",
              "                                               Tweet  \\\n",
              "0  अमेरिकामा कोभिड बाट एकै दिन चार हजारभन्दा बढीक...   \n",
              "1  कोभिड का कारण विदेशमा रहेका नेपालीहरुमा मानसिक...   \n",
              "2  नेपालमा क्लोभर बायोफार्मास्युटिकल्स अस्ट्रेलिय...   \n",
              "3                            कोभिड को खोप पनि लगाइयो   \n",
              "4  अमेरिकामा कोभिड को नयाँ रेकर्ड एकै दिन हजारभन्...   \n",
              "\n",
              "                                      Tokanize_tweet  \n",
              "0  अमेरिकामा,कोभिड,बाट,एकै,दिन,चार,हजारभन्दा,बढीक...  \n",
              "1  कोभिड,का,कारण,विदेशमा,रहेका,नेपालीहरुमा,मानसिक...  \n",
              "2  नेपालमा,क्लोभर,बायोफार्मास्युटिकल्स,अस्ट्रेलिय...  \n",
              "3                            कोभिड,को,खोप,पनि,लगाइयो  \n",
              "4  अमेरिकामा,कोभिड,को,नयाँ,रेकर्ड,एकै,दिन,हजारभन्...  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('../data/covid19_tweeter_dataset.csv', encoding='utf-8')\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/tilak/projects/tilak/mds2/.venv/bin/python\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.executable)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "N2brERKluO-a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 33474 entries, 0 to 33473\n",
            "Data columns (total 5 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   Unnamed: 0      33474 non-null  int64 \n",
            " 1   Label           33474 non-null  object\n",
            " 2   Datetime        33474 non-null  object\n",
            " 3   Tweet           33474 non-null  object\n",
            " 4   Tokanize_tweet  33471 non-null  object\n",
            "dtypes: int64(1), object(4)\n",
            "memory usage: 1.3+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yXRwcHxfoOR2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Label</th>\n",
              "      <th>Tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1</td>\n",
              "      <td>अमेरिकामा कोभिड बाट एकै दिन चार हजारभन्दा बढीक...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1</td>\n",
              "      <td>कोभिड का कारण विदेशमा रहेका नेपालीहरुमा मानसिक...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>नेपालमा क्लोभर बायोफार्मास्युटिकल्स अस्ट्रेलिय...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>कोभिड को खोप पनि लगाइयो</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1</td>\n",
              "      <td>अमेरिकामा कोभिड को नयाँ रेकर्ड एकै दिन हजारभन्...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Label                                              Tweet\n",
              "0    -1  अमेरिकामा कोभिड बाट एकै दिन चार हजारभन्दा बढीक...\n",
              "1    -1  कोभिड का कारण विदेशमा रहेका नेपालीहरुमा मानसिक...\n",
              "2     1  नेपालमा क्लोभर बायोफार्मास्युटिकल्स अस्ट्रेलिय...\n",
              "3     0                            कोभिड को खोप पनि लगाइयो\n",
              "4    -1  अमेरिकामा कोभिड को नयाँ रेकर्ड एकै दिन हजारभन्..."
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = df.drop(columns=['Unnamed: 0', 'Datetime', 'Tokanize_tweet'], axis=1)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Le9T7ryUunr7"
      },
      "source": [
        "## Data Pre-Processing\n",
        "\n",
        "**Tokenize**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "DqNUNu6Fudpq"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'nltk'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstopwords\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp3IEBge4cRo"
      },
      "outputs": [],
      "source": [
        "# df['tokenized_text'] = df['text'].apply(nltk.word_tokenize)\n",
        "df['tokenized_text'] = df['Tweet'].map(nltk.word_tokenize)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qsDs1fD6JU4"
      },
      "source": [
        "**Stop Word Removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vvm7V4M6Q7S"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98Ut2Irz6TsZ"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nepali_stopwords = stopwords.words('nepali')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HEcHEoHL6g5Z"
      },
      "outputs": [],
      "source": [
        "# nepali_stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlRskrzZ6QtC"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in nepali_stopwords]\n",
        "\n",
        "df['tokenized_text_no_stopwords'] = df['tokenized_text'].apply(remove_stopwords)\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO0xRxn76Qe7"
      },
      "source": [
        "### Designing Stemmer for removing the suffixes\n",
        "\n",
        "- Takes a list of tokens as input.\n",
        "- Initializes an empty list stemmed_tokens to store stemmed tokens.\n",
        "- Defines a list of suffixes to remove:`['मा', 'बाट', 'को', 'हरु']`\n",
        "- Iterates through each token:\n",
        "\n",
        "    - For each suffix, checks if the token ends with it using endswith.\n",
        "    - If a match is found, removes the suffix using slicing token`[:-len suffix)]`.\n",
        "    - break is used to stop checking further suffixes for the current token after a suffix is removed.\n",
        "    - Appends the stemmed token to `stemmed_tokens`.\n",
        "\n",
        "- Returns the list of stemmed tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PCDRaFW7DV8"
      },
      "outputs": [],
      "source": [
        "SUFFIXES = ['मा', 'बाट', 'को', 'का', 'हरु']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5ITr-R97U6c"
      },
      "outputs": [],
      "source": [
        "def rule_based_stemmer(tokens):\n",
        "    stemmed_tokens = []\n",
        "    for token in tokens:\n",
        "        for suffix in SUFFIXES:\n",
        "            if token.endswith(suffix):\n",
        "                token = token[:-len(suffix)]\n",
        "                break  # Move to the next token after removing a suffix\n",
        "        stemmed_tokens.append(token)\n",
        "    return stemmed_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldBLoG0d7qAo"
      },
      "outputs": [],
      "source": [
        "df['stemmed_tokens'] = df['tokenized_text_no_stopwords'].apply(rule_based_stemmer)\n",
        "\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlx1ajY97uFB"
      },
      "source": [
        "## Preparing Datasets for Training/Validation\n",
        "\n",
        "We split the entire dataset into two parts: `training set` and `testing set`.\n",
        "- The proportion of training and testing sets may depend on the corpus size.\n",
        "- In the train-test split, make sure the the distribution of the classes is proportional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPzNs-iP8P0p"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_test = train_test_split(df, test_size = 0.20, random_state=42)\n",
        "\n",
        "print(f\"Train set size: {len(df_train)}\")\n",
        "print(f\"Test set size: {len(df_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOUiZVSw9rsT"
      },
      "source": [
        "## Vectorize (TF-IDF)\n",
        "\n",
        "\n",
        "**TF-IDF** stands for Term Frequency-Inverse Document Frequency.\n",
        "\n",
        "It's a numerical statistic used in Natural Language Processing to reflect how important a word is to a document within a collection of documents (corpus).\n",
        "\n",
        "It works by considering two factors:\n",
        "\n",
        "- **Term Frequency (TF)**: How frequently a word appears in a document. Higher frequency generally means higher importance.\n",
        "\n",
        "- **Inverse Document Frequency (IDF)**: How common or rare a word is across the entire corpus. Words that appear in many documents are less important than words that appear in a few.\n",
        "\n",
        "$TFIDF(t, d, D) = TF(t, d) \\times IDF(t, D) $\n",
        "\n",
        "\n",
        "Where:\n",
        "- $t$ represents the term (word)\n",
        "- $d$ represents the document\n",
        "- $D$ represents the corpus (collection of documents)\n",
        "\n",
        "\n",
        "\n",
        "We will use TF-IDF for vectorization\n",
        "\n",
        "To create a `bag-of-words` based `TF-IDF` vector from the stemmed_text column, use the `TfidfVectorizer` from `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQPW_8Tb7tTg"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hLkhfQ1-B42"
      },
      "outputs": [],
      "source": [
        "# Join the stemmed tokens back into sentences\n",
        "df['stemmed_token_joined'] = df['stemmed_tokens'].apply(' '.join)\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# fit the vectorizer\n",
        "vectorizer.fit(df['stemmed_token_joined'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDQ3nOXK_lps"
      },
      "outputs": [],
      "source": [
        "vector = vectorizer.transform([\"कोभिड समस्या पारेको छ\"])\n",
        "print(vector.toarray())\n",
        "vector.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq1q_xuL_AaP"
      },
      "source": [
        "Vectorize train and test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3VBwJQe_GGA"
      },
      "outputs": [],
      "source": [
        "df_train['stemmed_token_joined'] = df_train['stemmed_tokens'].apply(' '.join)\n",
        "X_train_bow = vectorizer.transform(df_train['stemmed_token_joined'])\n",
        "\n",
        "df_test['stemmed_token_joined'] = df_test['stemmed_tokens'].apply(' '.join)\n",
        "X_test_bow  = vectorizer.transform(df_test['stemmed_token_joined'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Fu_akcBCPN0"
      },
      "outputs": [],
      "source": [
        "X_train_bow.shape, X_test_bow.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNwyvszmCWaK"
      },
      "source": [
        "## Visualize the vector\n",
        "\n",
        "To visualize the TF-IDF matrix, you can use dimensionality reduction techniques like `PCA` or `t-SNE` to project the high-dimensional matrix into a 2D or 3D space.\n",
        "\n",
        "Then, you can plot the projected data using libraries like matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4DKDGZ9CV7r"
      },
      "outputs": [],
      "source": [
        "import plotly.express as px\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGmU03NcCkO7"
      },
      "source": [
        "Then, apply PCA to reduce the dimensionality of the TF-IDF matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCHs0k7GCmYZ"
      },
      "outputs": [],
      "source": [
        "pca = PCA(n_components=3)  # Reduce to 3 dimensions for visualization\n",
        "reduced_tfidf = pca.fit_transform(X_test_bow.toarray())  # Convert sparse matrix to dense array\n",
        "reduced_tfidf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eg6aNaroC2op"
      },
      "outputs": [],
      "source": [
        "# df_3d = pd.DataFrame(reduced_tfidf, columns=['PC1', 'PC2', 'PC3'])\n",
        "\n",
        "# fig = px.scatter_3d(df_3d, x='PC1', y='PC2', z='PC3')\n",
        "# fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SlbXntoDq9-"
      },
      "source": [
        "Generate labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "791SWx9fEUBj"
      },
      "outputs": [],
      "source": [
        "y_train = df_train['Label']\n",
        "y_test = df_test['Label']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ct0C3Z0-EhGY"
      },
      "source": [
        "## Model Training\n",
        "\n",
        "For our sentiment classifier, we will try a few common classification algorithms:\n",
        "\n",
        "- Support Vector Machine\n",
        "- Decision Tree\n",
        "- Naive Bayes\n",
        "- Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evFdLt67E0cj"
      },
      "outputs": [],
      "source": [
        "# from sklearn import svm\n",
        "\n",
        "# model_svm = svm.SVC(C=8.0, kernel='linear')\n",
        "# model_svm.fit(X_train_bow, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22u6v9xuFGUm"
      },
      "source": [
        "Cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X12O-7JAE_Po"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import cross_val_score\n",
        "# model_svm_acc = cross_val_score(estimator=model_svm, X=X_train_bow, y=y_train, cv=5, n_jobs=-1)\n",
        "# model_svm_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgmUz21wHLCi"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model_dec = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
        "model_dec.fit(X_train_bow, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L22ZP34nHsPa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "model_dec_acc = cross_val_score(estimator=model_dec, X=X_test_bow, y=y_test, cv=5, n_jobs=2)\n",
        "model_dec_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wIIIHwJFxot"
      },
      "source": [
        "## Evaluation of Model\n",
        "To evaluate each model's performance, there are several common metrics can be used.\n",
        "\n",
        "- Precision\n",
        "- Recall\n",
        "- F-score\n",
        "- Accuracy\n",
        "- Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxNrLfExIOFL"
      },
      "outputs": [],
      "source": [
        "# Mean Accuracy\n",
        "print(model_dec.score(X_test_bow, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeJLDTIaIXW2"
      },
      "outputs": [],
      "source": [
        "# F1\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_pred = model_dec.predict(X_test_bow)\n",
        "\n",
        "f1_score(y_test, y_pred,\n",
        "         average=None,\n",
        "         labels = [1,-1,0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwyI8qb2JOpT"
      },
      "source": [
        "## Inference\n",
        "\n",
        "- define input\n",
        "- process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zinTY2mOmww"
      },
      "outputs": [],
      "source": [
        "# df_train.head(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rB2DIp2FJ71i"
      },
      "outputs": [],
      "source": [
        "tweet = \"नेपालमा कोभिड बढ्नु राम्रो कुरा हो\"\n",
        "tweet = \"नेपालमा कोभिड बढ्नु नराम्रो कुरा हो\"\n",
        "\n",
        "tweet = \"कोभिडले सप्तरीमा संक्रमितको लक्षण देखिएका तीनजना मध्धे एक जानाको मृत्यु भएको छ\"\n",
        "\n",
        "tokens          = nltk.word_tokenize(tweet)\n",
        "tokens          = remove_stopwords(tokens)\n",
        "stemmed_tokens  = rule_based_stemmer(tokens)\n",
        "stemmed_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHBW8MB_N6dW"
      },
      "source": [
        "Now Vectorize and pass to model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ue-iRkeN0L4"
      },
      "outputs": [],
      "source": [
        "X_feature = vectorizer.transform([' '.join(stemmed_tokens)])\n",
        "X_feature.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlQYgI4cOPVp"
      },
      "outputs": [],
      "source": [
        "model_dec.predict(X_feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9IUXswUSiCf"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDGtFcUgSkGg"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv3EyomrSwVu"
      },
      "source": [
        "Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBE3F7DdSmxo"
      },
      "outputs": [],
      "source": [
        "with open('model_dec.pkl', 'wb') as file:\n",
        "    pickle.dump(model_dec, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBDOYu-rSyVn"
      },
      "source": [
        "Inference with saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7_gHO5CS1vK"
      },
      "outputs": [],
      "source": [
        "with open('model_dec.pkl', 'rb') as file:\n",
        "    model_dec_loaded = pickle.load(file)\n",
        "    pred = model_dec.predict(X_feature)\n",
        "print(f\"prediction from saved model: {pred}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6hFIlioYdWb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_pred = model_dec.predict(X_test_bow)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JgxI5ZubeXF"
      },
      "source": [
        "# Assignment - Enhancing the Sentiment Analysis System\n",
        "\n",
        "Now extend/enhance this notebook to achive following-\n",
        "\n",
        "**Task 1**: Train following classifiers and Compute the `F1`, `Accuracy` of the model\n",
        "- Support Vector Machine\n",
        "- Decision Tree\n",
        "- Logistic Regression\n",
        "\n",
        "Write the interpretation of the finding by comparing the model accuracy.\n",
        "\n",
        "**Task 2**: Perform the hyperparameter tuning on each of the model (from task 1)\n",
        "- Prepare a table showing the parameters for each classification model\n",
        "\n",
        "**Task3**: Feature Engineering\n",
        "- Instead of using `uni-gram` token on TF-IDF, use `bi-gram` token\n",
        "- Train the decision tree model\n",
        "- Hyperparameter tune the model to ensure the higher accuracy\n",
        "\n",
        "**Deliverables**\n",
        "1. Python Notebook\n",
        "2. Your Best model from ***Task 3*** (saved pickel file)\n",
        "\n",
        "\n",
        "\n",
        "**Evaluation**\n",
        "- Your model should have accuracy `more than 70%` to get the full marks\n",
        "- The leader board will be published\n",
        "- Top 10% in the leader board will get the additional bonus marks\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
